{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Announcements\n",
    "* TAs are in the process of grading your assignment\n",
    "* Block IV is online, will start on Thursday (please update material)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(str.split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'   a b c   '.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(['b', 'c', 'a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'aa', 'aaa']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(['aa', 'aaa', 'a'], key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '?', 'a', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'p', 'r', 't', 'w', 'y']\n"
     ]
    }
   ],
   "source": [
    "def split_sort_text(text):\n",
    "    text = text.replace(' ','') #remove space between words\n",
    "    a_set = set(text)  #create a set\n",
    "    return(sorted(a_set)) #sort alphabetically the letters and return\n",
    "\n",
    "text = 'what happened, my friend?'\n",
    "x = split_sort_text(text)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['friend?', 'happened,', 'my', 'what']\n"
     ]
    }
   ],
   "source": [
    "def split_sort_text (text):\n",
    "    '''This function splits the input (a string) on the spaces and returns\n",
    "        a list of all unique words in alphabetical order'''\n",
    "    \n",
    "    splitted_text = text.split(' ')\n",
    "    set_of_words = set(splitted_text)\n",
    "    return sorted(list(set_of_words))\n",
    "\n",
    "text = 'what happened, my friend?'\n",
    "x = split_sort_text(text)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['an', 'example', 'for', 'function', 'is', 'testing', 'text', 'this']\n"
     ]
    }
   ],
   "source": [
    "def split_sort_text(text):\n",
    "    \"\"\"\n",
    "    Split text into words at white spaces and \n",
    "    return the vocabulary in alphabetical order\n",
    "    \"\"\"\n",
    "    words = text.split() # or text.split(sep=' ')\n",
    "    set_words = set(words)\n",
    "    sorted_set_words = sorted(set_words)\n",
    "    \n",
    "    return sorted_set_words\n",
    "\n",
    "return_value = split_sort_text('this is an    example text for testing this function')\n",
    "print(return_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "**google**: \"nltk wordnet synset definitions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method synsets in module nltk.corpus.reader.wordnet:\n",
      "\n",
      "synsets(lemma, pos=None, lang='eng', check_exceptions=True) method of nltk.corpus.reader.wordnet.WordNetCorpusReader instance\n",
      "    Load all synsets with a given lemma and part of speech tag.\n",
      "    If no pos is specified, all synsets for all parts of speech\n",
      "    will be loaded.\n",
      "    If lang is specified, all the synsets associated with the lemma name\n",
      "    of that language will be returned.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(wn.synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> [Synset('dog.n.01'), Synset('frump.n.01'), Synset('dog.n.03'), Synset('cad.n.01'), Synset('frank.n.02'), Synset('pawl.n.01'), Synset('andiron.n.01'), Synset('chase.v.01')]\n"
     ]
    }
   ],
   "source": [
    "return_value = wn.synsets('dog')\n",
    "print(type(return_value), return_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds\n"
     ]
    }
   ],
   "source": [
    "synset1 = return_value[0]\n",
    "print(synset1.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(return_value[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "* **count_words.py**\n",
    "* **utils_3a.py**\n",
    "\n",
    "To show:\n",
    "* preprocess not imported or wrongly\n",
    "* what does testing mean\n",
    "* steps according to google doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'one': 2, 'two': 1, 'three': 1})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import utils_3a\n",
    "from utils_3a import preprocess \n",
    "\n",
    "def count(words):\n",
    "    \"\"\"Counts words after cleaning text\"\"\"\n",
    "    from collections import Counter\n",
    "    words = preprocess(words)\n",
    "    freq = Counter(words)\n",
    "    return freq\n",
    "\n",
    "count('one two one three')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed...\n",
      "this is a  tricky  test\n"
     ]
    }
   ],
   "source": [
    "def preprocess(unprocessed_string):\n",
    "    \"\"\"\n",
    "    Will remove all unwanted characters, in this case everything except alphanumberical chars\n",
    "    \"\"\"\n",
    "    for character in set(unprocessed_string):\n",
    "        if not character.isalnum():\n",
    "            unprocessed_string = unprocessed_string.replace(character,\" \")\n",
    "        \n",
    "    print(\"preprocessed...\")\n",
    "    return unprocessed_string\n",
    "\n",
    "return_value = preprocess('this is a (tricky) test')\n",
    "print(return_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'~', '|', ')', '#', '@', '<', '\\\\', ';', ']', '+', '-', ':', '&', '^', '`', '/', '?', '=', '$', '_', '%', '{', '>', '\"', '!', '}', '.', '*', '(', '[', ',', \"'\"}\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "punctuation = set(string.punctuation)\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Formatter',\n",
       " 'Template',\n",
       " '_ChainMap',\n",
       " '_TemplateMetaclass',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_re',\n",
       " '_string',\n",
       " 'ascii_letters',\n",
       " 'ascii_lowercase',\n",
       " 'ascii_uppercase',\n",
       " 'capwords',\n",
       " 'digits',\n",
       " 'hexdigits',\n",
       " 'octdigits',\n",
       " 'printable',\n",
       " 'punctuation',\n",
       " 'whitespace']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"I Am The Walrus\"\\n(\"Magical Mystery Tour\" Version)\\n\\nI am he\\nAs you are he\\nAs you are me\\nAnd we are all together\\n\\nSee how they run\\nLike pigs from a gun\\nSee how they fly\\nI\\'m crying\\n\\nSitting on a cornflake\\nWaiting for the van to come\\nCorporation tee shirt\\nStupid bloody Tuesday\\nMan, you been a naughty boy\\nYou let your face grow long\\n\\nI am the eggman (Ooh)\\nThey are the eggmen, (Ooh)\\nI am the walrus\\nGoo goo g\\' joob\\n\\nMister city p\\'liceman sitting pretty\\nLittle p\\'licemen in a row\\nSee how they fly\\nLike Lucy in the sky\\nSee how they run\\nI\\'m crying\\nI\\'m crying, I\\'m crying, I\\'m crying\\n\\nYellow matter custard\\nDripping from a dead dog\\'s eye\\nCrabalocker fishwife pornographic priestess\\nBoy you been a naughty girl\\nYou let your knickers down\\n\\nI am the eggman (Ooh)\\nThey are the eggmen (Ooh)\\nI am the walrus\\nGoo goo g\\' joob\\n\\nSitting in an English\\nGarden waiting for the sun\\nIf the sun don\\'t come\\nYou get a tan from standing in the English rain\\n\\nI am the eggman\\nThey are the eggmen\\nI am the walrus\\nGoo goo g\\' joob g\\' goo goo g\\' joob\\n\\nExpert texpert choking smokers\\nDon\\'t you think the joker laughs at you?\\nSee how they smile\\nLike pigs in a sty, see how they snied\\nI\\'m crying\\n\\nSemolina pilchards\\nClimbing up the Eiffel Tower\\nElement\\'ry penguin singing Hare Krishna\\nMan, you should have seen them kicking Edgar Allan Poe\\n\\nI am the eggman (Ooh)\\nThey are the eggmen (Ooh)\\nI am the walrus\\nGoo goo g\\' joob\\nGoo goo g\\' joob\\nG\\' goo goo g\\' joob\\nGoo goo g\\' joob, goo goo g\\' goo g\\' goo goo g\\' joob joob\\nJoob joob...'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_text(filepath):\n",
    "    \n",
    "    \"\"\"function that opens and reads a file and returns the text in the file\"\"\"\n",
    "    \n",
    "    with open(filepath, \"r\") as infile:\n",
    "        content = infile.read()\n",
    "    \n",
    "    return content\n",
    " \n",
    "return_value = load_text(\"../Data/lyrics/walrus.txt\")\n",
    "return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_walrus():\n",
    "    \"\"\" function which takes lyrics as input and replaces every instance of 'walrus' by 'hippo' and save it to a text file\"\"\"\n",
    "    \n",
    "    filepath = \"../Data/lyrics/walrus.txt\"\n",
    "\n",
    "    with open(filepath, \"r\") as infile:\n",
    "        content = infile.read()  \n",
    "        new_content = content.lower().replace('walrus', 'hippo') #replace text\n",
    "        \n",
    "        \n",
    "       # print(new_content)            ---   !!!!UNCOMMENT TO SEE IT PRINTED!!!!    ---\n",
    "        \n",
    "        #Save into a new file\n",
    "        filename = \"../Data/lyrics/walrus_hippo.txt\"\n",
    "        with open(filename, \"w\") as outfile:\n",
    "            outfile.write(new_content)\n",
    "    \n",
    "    \n",
    "replace_walrus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(path):\n",
    "    \"\"\"\n",
    "    Read file and return text\n",
    "    \"\"\"\n",
    "    with open(path, mode='r') as infile:\n",
    "        text = infile.read()\n",
    "    return text\n",
    "\n",
    "def replace_walrus(lyrics):\n",
    "    lyrics = lyrics.replace('Walrus', 'Hippo')\n",
    "    lyrics = lyrics.replace('walrus', 'hippo')\n",
    "    \n",
    "    with open('../Data/lyrics/walrus_hippo.txt', 'w') as outfile:\n",
    "        outfile.write(lyrics)\n",
    "    \n",
    "\n",
    "input_path = '../Data/lyrics/walrus.txt' \n",
    "lyrics = load_text(input_path)\n",
    "replace_walrus(lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def pos_tag(text):\n",
    "    \"\"\"\n",
    "    Lemmatize all the nouns in a given text\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos_tagged = nltk.pos_tag(tokens)\n",
    "      \n",
    "    return pos_tagged\n",
    "\n",
    "#pos_tag(test_text)\n",
    "\n",
    "test_text = \"\"\"Two households, both alike in dignity,\n",
    "    In fair Verona, where we lay our scene,\n",
    "    From ancient grudge break to new mutiny,\n",
    "    Where civil blood makes civil hands unclean.\"\"\"\n",
    "result = pos_tag(test_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6a: nested for loops\n",
    "More than you will probably ever need "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6b: \n",
    "* **w**: created new empty file and points file pointer at first character of the file. Makes it possible to write information to file\n",
    "* **r**: points file pointer to first character of file. Makes it possible to read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../Data/books/AnnaKarenina.txt', '../Data/books/HuckFinn.txt', '../Data/books/Macbeth.txt']\n"
     ]
    }
   ],
   "source": [
    "import glob \n",
    "\n",
    "def get_paths (input_folder):\n",
    "    '''This function returns a list of all the paths to a txt file in a certain folder'''\n",
    "\n",
    "    input_folder_txt = input_folder + '/*.txt'\n",
    "    list_of_paths =  glob.glob(input_folder_txt)\n",
    "    return list_of_paths\n",
    "\n",
    "paths = get_paths('../Data/books')\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> ['../Data/books/AnnaKarenina.txt', '../Data/books/HuckFinn.txt', '../Data/books/Macbeth.txt']\n"
     ]
    }
   ],
   "source": [
    "paths = glob.glob('../Data/books/*.txt')\n",
    "print(type(paths), paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_paths(input_folder):\n",
    "    \"\"\"\n",
    "    Store all paths to .txt files in the input_folder in a list.\n",
    "    Return a list of strings, i.e., each string is a file path\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "    for filename in glob.glob(input_folder + \"/*\"):\n",
    "        paths.append(filename)\n",
    "    return paths\n",
    "\n",
    "# Exercise 1b\n",
    "# call analyze.py from the command line\n",
    "# command line input:\n",
    "# cd /Users/quincy/Google\\Drive/VU/Python/python-for-text-analysis-master/Assignments/ASSIGNMENT_3\n",
    "# python analyze.py   \n",
    "# command line output: \n",
    "# ['../../Data/books/AnnaKarenina.txt', '../../Data/books/HuckFinn.txt', '../../Data/books/Macbeth.txt']\n",
    "# It works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function listdir in module posix:\n",
      "\n",
      "listdir(path=None)\n",
      "    Return a list containing the names of the files in the directory.\n",
      "    \n",
      "    path can be specified as either str, bytes, or a path-like object.  If path is bytes,\n",
      "      the filenames returned will also be bytes; in all other circumstances\n",
      "      the filenames returned will be str.\n",
      "    If path is None, uses the path='.'.\n",
      "    On some platforms, path may also be specified as an open file descriptor;\\\n",
      "      the file descriptor must refer to a directory.\n",
      "      If this functionality is unavailable, using it raises NotImplementedError.\n",
      "    \n",
      "    The list is in arbitrary order.  It does not include the special\n",
      "    entries '.' and '..' even if they are present in the directory.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(os.listdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_sents': 614,\n",
       " 'num_tokens': 13886,\n",
       " 'vocab_size': 2754,\n",
       " 'num_chapters_or_acts': 5}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import glob\n",
    " \n",
    "def get_basic_stats(txt_path):\n",
    "    \"\"\"\n",
    "    computes the statistics for a text file\n",
    "\t\n",
    "    :param filepath txt_path: the text file from which the statistics have to be computed\n",
    "\t\n",
    "    :rtype: dict\n",
    "    :return: a dictionary with with the names of the statistics as keys and the number of each statistic as values\n",
    "    \"\"\"\n",
    "    for filename in glob.glob(txt_path): # reading file and assign the content to a variable\n",
    "        with open(filename, \"r\", encoding='utf-8') as infile:\n",
    "            content = infile.read()  \n",
    "            \n",
    "        stats_text = dict()\n",
    "        sentences = sent_tokenize(content) # splitting the content into sentences for count\n",
    "        stats_text['num_sents'] = len(sentences)\n",
    "\n",
    "        tokens= word_tokenize(content) # splitting the content into tokens for count\n",
    "        stats_text['num_tokens'] = len(tokens)\n",
    "\n",
    "        unique_voc = content.lower() # making text lowercase to get a good alphabetical order\n",
    "        unique_voc = unique_voc.split()\n",
    "        unique_voc = sorted(set(unique_voc)) # turning the list into a set to remove duplicates & by sorting it, turning the set into a list\n",
    "        unique_voc_len = len(unique_voc) # count for unique words in vocabulary\n",
    "        stats_text['vocab_size'] = unique_voc_len\n",
    "\n",
    "        nr_chapters = content.count('CHAPTER') or content.count('Chapter') or content.count('ACT') # count for different names in books for chapter\n",
    "        stats_text['num_chapters_or_acts'] = nr_chapters\n",
    "    \n",
    "    return stats_text\n",
    "\n",
    "get_basic_stats('../Data/books/HuckFinn.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0 or 1 or 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    " \n",
    "def get_basic_stats(txt_path):\n",
    "    \"\"\"\n",
    "  \n",
    "    The number of sentences\n",
    "    The number of tokens\n",
    "    The size of the vocabulary used (i.e. unique tokens)\n",
    "    the number of chapters/acts\n",
    "    \n",
    "    \"\"\"\n",
    "    with open(txt_path,\"r\") as infile:\n",
    "        content = infile.read()\n",
    "        num_sents = len(sent_tokenize(content)) #sentence number\n",
    "        num_tokens = len(word_tokenize(content))#tokens number\n",
    "        vocab_size = len(set(word_tokenize(content))) #uniquewords number\n",
    "    \n",
    "    basename = os.path.basename(txt_path)\n",
    "    book = basename.strip('.txt')\n",
    "    if book == \"HuckFinn\":\n",
    "        num_chapters_or_acts = content.count('CHAPTER')\n",
    "    elif book == \"AnnaKarenina\":\n",
    "        num_chapters_or_acts = content.count('Chapter ')\n",
    "    elif book == \"Macbeth\":\n",
    "        num_chapters_or_acts = content.count('ACT')\n",
    "    \n",
    "    basic_stats = {'num_sents':num_sents,\n",
    "                   'num_tokens':num_tokens,\n",
    "                   'vocab_size':vocab_size,\n",
    "                   'num_chapters_or_acts': num_chapters_or_acts}\n",
    "    return basic_stats\n",
    " \n",
    "txt_path = '../Data/books/HuckFinn.txt'\n",
    "print(get_basic_stats(txt_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import os\n",
    "\n",
    "novels = []\n",
    "\n",
    "book2stats = {}\n",
    "input_folder = '../Data/books'\n",
    "paths = utils.get_paths(input_folder)\n",
    "book2stats = {}\n",
    "for path in paths:\n",
    "    book = os.path.basename(path)\n",
    "    book = book.strip('.txt')\n",
    "    stats = utils.get_basic_stats(path)\n",
    "    book2stats[path] = stats\n",
    "\n",
    "number_sents = []\n",
    "number_tokens = []\n",
    "vocabulary_size = []\n",
    "number_chapters_or_acts = []\n",
    "\n",
    "# creating lists with values for each statistic (key)\n",
    "for novel in book2stats:\n",
    "    novels.append(novel)\n",
    "    number_sents.append(book2stats[novel]['num_sents'])\n",
    "    number_tokens.append(book2stats[novel]['num_tokens'])\n",
    "    vocabulary_size.append(book2stats[novel]['vocab_size'])\n",
    "    number_chapters_or_acts.append(book2stats[novel]['num_chapters_or_acts'])\n",
    "\n",
    "# determining the book with the highest value for each statistic (key).\n",
    "# the index of the highest values in the lists above correspond to a specifix index of the list with novels.\n",
    "num_sents = novels[number_sents.index(max(number_sents))]\n",
    "num_tokens = novels[number_tokens.index(max(number_tokens))]\n",
    "vocab_size = novels[vocabulary_size.index(max(vocabulary_size))]\n",
    "num_chapters_or_acts = novels[number_chapters_or_acts.index(max(number_chapters_or_acts))]\n",
    "\n",
    "# adding keys and highest values to dictionary\n",
    "stats2book_with_highest_value = dict()\n",
    "stats2book_with_highest_value['num_sents'] = num_sents\n",
    "stats2book_with_highest_value['num_tokens'] = num_tokens\n",
    "stats2book_with_highest_value['vocab_size'] = vocab_size\n",
    "stats2book_with_highest_value['num_chapters_or_acts'] = num_chapters_or_acts\n",
    "\n",
    "stats2book_with_highest_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [1,3,5,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two ways of finding the highest value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "max_value = max(values)\n",
    "max_index = values.index(max_value)\n",
    "\n",
    "print(max_index)\n",
    "print(max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "1 3\n",
      "2 5\n",
      "2\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "max_index = -100\n",
    "max_value = -100\n",
    "\n",
    "values = [1,3,5,2]\n",
    "for value in values:\n",
    "    index = values.index(value)\n",
    "    \n",
    "    if value > max_value:\n",
    "        max_value = value\n",
    "        max_index = index\n",
    "        print(max_index, max_value)\n",
    "\n",
    "print(max_index)\n",
    "print(max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "five\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "a_dict = {'one' : 1,\n",
    "          'three': 3,\n",
    "          'five' : 5,\n",
    "          'two' : 2}\n",
    "\n",
    "values = a_dict.values()\n",
    "\n",
    "max_key = None\n",
    "max_value = max(values)\n",
    "\n",
    "for key, value in a_dict.items():\n",
    "    \n",
    "    if value == max_value:\n",
    "        max_key = key\n",
    "        max_value = value\n",
    "\n",
    "print(max_key)\n",
    "print(max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "five\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "a_dict = {'one' : 1,\n",
    "          'three': 3,\n",
    "          'five' : 5,\n",
    "          'two' : 2}\n",
    "\n",
    "max_key = None\n",
    "max_value = -100\n",
    "\n",
    "for key, value in a_dict.items(): \n",
    "    \n",
    "    if value >= max_value:\n",
    "        max_key = key\n",
    "        max_value = value\n",
    "\n",
    "print(max_key)\n",
    "print(max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'one'), (3, 'three'), (5, 'five'), (2, 'two')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(5, 'five'), (3, 'three'), (2, 'two'), (1, 'one')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_dict = {'one' : 1,\n",
    "          'three': 3,\n",
    "          'five' : 5,\n",
    "          'two' : 2}\n",
    "\n",
    "value_key = []\n",
    "for key, value in a_dict.items():\n",
    "    value_key.append((value, key))\n",
    "    \n",
    "print(value_key)\n",
    "\n",
    "sorted(value_key, reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
